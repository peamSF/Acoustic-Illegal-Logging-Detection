# -*- coding: utf-8 -*-
"""Code.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10e650_0lkYy0El1-5B6u3BXdAoreUxkE
"""

import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import librosa
import librosa.display
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, fbeta_score, f1_score
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')


CHAINSAW_PATH = r"D:\AI_Builders2025\Ver5_Dataset\ChainsawSound"
NON_CHAINSAW_PATH = r"D:\AI_Builders2025\Ver5_Dataset\Non-Chainsaw"


# Parameters for feature extraction
SAMPLE_RATE = 22050
DURATION = 10  # sec
N_MFCC = 40
N_FFT = 2048  # length of the FFT window
HOP_LENGTH = 512  # number of samples between frames


# Extract MFCC features from audio files
def extract_features(file_path):
    try:
        # For WAV files
        audio, sr = librosa.load(file_path, sr=SAMPLE_RATE, duration=DURATION)

        # Ensure consistent length by padding or trimming
        if len(audio) < SAMPLE_RATE * DURATION:
            audio = np.pad(audio, (0, SAMPLE_RATE * DURATION - len(audio)), 'constant')
        else:
            audio = audio[:SAMPLE_RATE * DURATION]

        # Extract MFCC features
        mfcc = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=N_MFCC, n_fft=N_FFT, hop_length=HOP_LENGTH)

        # Normalize MFCC
        mfcc = (mfcc - np.mean(mfcc)) / np.std(mfcc)

        return mfcc

    except Exception as e:
        print(f"Error processing {file_path}: {e}")
        return None


# Load and preprocess all data
def prepare_dataset():
    features = []
    labels = []
    file_paths = []

    print("Processing Chainsaw sounds...")
    # Process Chainsaw sounds
    for file_name in os.listdir(CHAINSAW_PATH):
        if file_name.lower().endswith('.wav'):
            file_path = os.path.join(CHAINSAW_PATH, file_name)
            print(f"  Extracting features from: {file_name}")
            mfcc = extract_features(file_path)
            if mfcc is not None:
                features.append(mfcc)
                labels.append('chainsaw')
                file_paths.append(file_path)

    print("Processing non-chainsaw sounds...")
    # Process non-chainsaw sounds
    for file_name in os.listdir(NON_CHAINSAW_PATH):
        if file_name.lower().endswith('.wav'):
            file_path = os.path.join(NON_CHAINSAW_PATH, file_name)
            print(f"  Extracting features from: {file_name}")
            mfcc = extract_features(file_path)
            if mfcc is not None:
                features.append(mfcc)
                labels.append('non-chainsaw')
                file_paths.append(file_path)

    # Convert lists to numpy arrays
    features = np.array(features)
    labels = np.array(labels)

    # Reshape features for CNN input (samples, height, width, channels)
    features = features.reshape(features.shape[0], features.shape[1], features.shape[2], 1)

    # Encode labels
    encoder = LabelEncoder()
    y_encoded = encoder.fit_transform(labels)
    y_categorical = to_categorical(y_encoded)

    return features, y_categorical, encoder.classes_, file_paths


# CNN model
def create_model(input_shape):
    model = Sequential([
        Conv2D(32, (3, 3), activation='relu', input_shape=input_shape),
        MaxPooling2D((2, 2)),

        Conv2D(64, (3, 3), activation='relu'),
        MaxPooling2D((2, 2)),

        Conv2D(128, (3, 3), activation='relu'),
        MaxPooling2D((2, 2)),

        Flatten(),
        Dense(128, activation='relu'),
        Dropout(0.5),
        Dense(2, activation='softmax')
    ])

    model.compile(
        optimizer='adam',
        loss='categorical_crossentropy',
        metrics=['accuracy']
    )

    return model


# Plot confusion matrix
def plot_confusion_matrix(y_true, y_pred, classes):
    cm = confusion_matrix(y_true, y_pred)
    plt.figure(figsize=(10, 8))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=classes, yticklabels=classes)
    plt.title('Confusion Matrix')
    plt.ylabel('True Label')
    plt.xlabel('Predicted Label')
    plt.tight_layout()
    plt.show()

    # Normalized confusion matrix
    cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
    plt.figure(figsize=(10, 8))
    sns.heatmap(cm_norm, annot=True, fmt='.2f', cmap='Blues', xticklabels=classes, yticklabels=classes)
    plt.title('Normalized Confusion Matrix')
    plt.ylabel('True Label')
    plt.xlabel('Predicted Label')
    plt.tight_layout()
    plt.show()


# Visualize spectrograms for 3 examples
def visualize_spectrograms(file_paths, labels, num_examples=3):
    chainsaw_paths = [p for p, l in zip(file_paths, labels) if l == 'chainsaw'][:num_examples]
    non_chainsaw_paths = [p for p, l in zip(file_paths, labels) if l == 'non-chainsaw'][:num_examples]

    # Plot spectrogram
    def plot_spec(file_path, ax, title):
        audio, sr = librosa.load(file_path, sr=SAMPLE_RATE, duration=DURATION)

        if audio is not None:
            # Generate mel-spectrogram
            S = librosa.feature.melspectrogram(y=audio, sr=sr, n_fft=N_FFT, hop_length=HOP_LENGTH)
            S_dB = librosa.power_to_db(S, ref=np.max)
            img = librosa.display.specshow(S_dB, x_axis='time', y_axis='mel', sr=sr, ax=ax)
            ax.set_title(title)
            return img

    # Create subplots
    fig, axes = plt.subplots(2, num_examples, figsize=(15, 8))

    # Plot chainsaw spectrograms
    for i, path in enumerate(chainsaw_paths):
        img = plot_spec(path, axes[0, i], f"Chainsaw {i+1}")

    # Plot non-chainsaw spectrograms
    for i, path in enumerate(non_chainsaw_paths):
        img = plot_spec(path, axes[1, i], f"Non-Chainsaw {i+1}")

    plt.tight_layout()
    fig.colorbar(img, ax=axes.ravel().tolist(), format='%+2.0f dB')
    plt.show()


# Main loop
if __name__ == "__main__":
    print("Extracting features from audio files...")
    X, y, class_names, file_paths = prepare_dataset()

    # Print dataset information
    print(f"Dataset shape: {X.shape}")
    print(f"Classes: {class_names}")
    print(f"Number of chainsaw samples: {np.sum(np.argmax(y, axis=1) == 0)}")
    print(f"Number of non-chainsaw samples: {np.sum(np.argmax(y, axis=1) == 1)}")

    # Create labels list for visualization
    labels_list = ['chainsaw' if np.argmax(y_i) == 0 else 'non-chainsaw' for y_i in y]

    # Visualize spectrograms for 3 samples from each class
    print("Generating spectrograms for sample files...")
    visualize_spectrograms(file_paths, labels_list, num_examples=3)

    # Visualize MFCC spectrograms for each class
    print("Visualizing MFCC spectrograms for each class...")
    # Get first sample from each class
    chainsaw_idx = np.where(np.argmax(y, axis=1) == 0)[0][0]
    non_chainsaw_idx = np.where(np.argmax(y, axis=1) == 1)[0][0]

    # Plot MFCC spectrograms
    plt.figure(figsize=(12, 5))
    plt.subplot(1, 2, 1)
    librosa.display.specshow(X[chainsaw_idx, :, :, 0], x_axis='time')
    plt.colorbar(format='%+2.0f dB')
    plt.title("Chainsaw MFCC")
    plt.ylabel('MFCC Coefficient')
    plt.xlabel('Time (frames)')

    plt.subplot(1, 2, 2)
    librosa.display.specshow(X[non_chainsaw_idx, :, :, 0], x_axis='time')
    plt.colorbar(format='%+2.0f dB')
    plt.title("Non-Chainsaw MFCC")
    plt.ylabel('MFCC Coefficient')
    plt.xlabel('Time (frames)')

    plt.tight_layout()
    plt.show()

    # Split the data into train, validation, and test (70:15:15)
    # First split: 70% train, 30% test
    X_train, X_temp, y_train, y_temp = train_test_split(
        X, y, test_size=0.3, random_state=42, stratify=np.argmax(y, axis=1))

    # Second split: split the test into validation and test (50% each of test, which is 15% of total)
    X_val, X_test, y_val, y_test = train_test_split(
        X_temp, y_temp, test_size=0.5, random_state=42, stratify=np.argmax(y_temp, axis=1))

    print(f"Training set size: {X_train.shape[0]} ({X_train.shape[0]/X.shape[0]*100:.1f}%)")
    print(f"Validation set size: {X_val.shape[0]} ({X_val.shape[0]/X.shape[0]*100:.1f}%)")
    print(f"Test set size: {X_test.shape[0]} ({X_test.shape[0]/X.shape[0]*100:.1f}%)")

    # Create and train the model
    input_shape = (X.shape[1], X.shape[2], 1)
    model = create_model(input_shape)

    # Print model summary
    model.summary()


    checkpoint = ModelCheckpoint(
        'Ver7_360_best_sound_classification_model.h5',
        monitor='val_accuracy',
        save_best_only=True,
        mode='max',
        verbose=1
    )

    early_stopping = EarlyStopping(
        monitor='val_loss',
        patience=10,
        restore_best_weights=True,
        verbose=1
    )

    # Train the model
    print("Training the model...")
    history = model.fit(
        X_train, y_train,
        epochs=50,
        batch_size=32,
        validation_data=(X_val, y_val),
        callbacks=[checkpoint, early_stopping]
    )

    # Evaluate the model on test set
    test_loss, test_acc = model.evaluate(X_test, y_test, verbose=2)
    print(f"\nTest accuracy: {test_acc:.2f}")

    # Plot accuracy and loss curves
    plt.figure(figsize=(12, 4))

    plt.subplot(1, 2, 1)
    plt.plot(history.history['accuracy'], label='Training Accuracy')
    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
    plt.title('Model Accuracy')
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy')
    plt.grid(True)
    plt.legend()

    plt.subplot(1, 2, 2)
    plt.plot(history.history['loss'], label='Training Loss')
    plt.plot(history.history['val_loss'], label='Validation Loss')
    plt.title('Model Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.grid(True)
    plt.legend()

    plt.tight_layout()
    plt.savefig('training_history.png')
    plt.show()

    # Generate predictions for evaluation
    y_pred_prob = model.predict(X_test)
    y_pred = np.argmax(y_pred_prob, axis=1)
    y_true = np.argmax(y_test, axis=1)
    f1 = f1_score(y_true, y_pred, average='weighted')
    f2 = fbeta_score(y_true, y_pred, beta=2, average='weighted')

    # Plot confusion matrix
    print("Generating confusion matrix...")
    plot_confusion_matrix(y_true, y_pred, class_names)

    # Generate classification report
    print("\nClassification Report:")
    report = classification_report(y_true, y_pred, target_names=class_names)
    print(report)
    print(f"F2-Score = {f2}")

    # Save the classification report to a file
    with open('Ver7_360_classification_report.txt', 'w') as f:
        f.write(f"Test Accuracy: {test_acc:.4f}\n\n")
        f.write(report)


    # Save the model
    model.save('Ver7_360_sound_classification_model.h5')
    print("Model saved as 'Ver7_360_sound_classification_model.h5'")

    # Function for manual test
    def predict_sound(file_path):
        # Extract features
        mfcc = extract_features(file_path)

        if mfcc is not None:
            # Reshape for manual test
            mfcc = mfcc.reshape(1, mfcc.shape[0], mfcc.shape[1], 1)

            # Predict
            prediction = model.predict(mfcc)[0]
            predicted_class = class_names[np.argmax(prediction)]
            confidence = np.max(prediction) * 100

            print(f"Prediction: {predicted_class} (Confidence: {confidence:.2f}%)")

            # Show original audio waveform
            audio, sr = librosa.load(file_path, sr=SAMPLE_RATE, duration=DURATION)

            # Create time axis in seconds
            time = np.linspace(0, len(audio) / sr, num=len(audio))
            plt.figure(figsize=(12, 4))
            plt.plot(time, audio)
            plt.title(f"Waveform - Predicted as {predicted_class}")
            plt.xlabel("Time (seconds)")
            plt.ylabel("Amplitude")
            plt.grid(True)
            plt.show()

            # Show Mel-Spectrogram
            S = librosa.feature.melspectrogram(y=audio, sr=sr, n_fft=N_FFT, hop_length=HOP_LENGTH)
            S_dB = librosa.power_to_db(S, ref=np.max)

            plt.figure(figsize=(10,6))
            librosa.display.specshow(S_dB, sr=sr, hop_length=HOP_LENGTH, x_axis='time', y_axis='mel')
            plt.colorbar(format='%+2.0f db')
            plt.title(f"Mel-Spectrogram - Predicted as {predicted_class}")
            plt.ylabel('Hz')
            plt.xlabel('Time')
            plt.tight_layout()
            plt.show()

            # Show MFCC spectrogram
            plt.figure(figsize=(10, 6))
            librosa.display.specshow(mfcc[0, :, :, 0], x_axis='time')
            plt.colorbar(format='%+2.0f dB')
            plt.title(f"MFCC Spectrogram - Predicted as {predicted_class}")
            plt.ylabel('MFCC Coefficient')
            plt.xlabel('Time (frames)')
            plt.tight_layout()
            plt.show()

            return predicted_class, confidence


    print("\nModel training and evaluation complete!")
    print("Summary of results:")
    print(f"- Test accuracy: {test_acc:.4f}")
    print("- Saved files:")
    print("  * Ver7_360_sound_classification_model.h5 (model)")
    print("  * Ver7_360_best_sound_classification_model.h5 (best model)")